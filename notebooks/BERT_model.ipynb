{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "# Set `PATH` to include the directory containing saved_model_cli\n",
        "PATH = %env PATH\n",
        "%env PATH=/home/jupyter/.local/bin:{PATH}"
      ],
      "metadata": {
        "id": "3tFneshPHs3L"
      },
      "id": "3tFneshPHs3L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jmyVfhB210-n"
      },
      "id": "jmyVfhB210-n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "\n",
        "\n",
        "#from official.nlp import optimization\n",
        "tf.get_logger().setLevel(\"ERROR\")"
      ],
      "metadata": {
        "id": "UJ7npcepJkkC"
      },
      "id": "UJ7npcepJkkC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/eminedemirbas/Hackathon-YZTA"
      ],
      "metadata": {
        "id": "oMD4Qji_J0pO"
      },
      "id": "oMD4Qji_J0pO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"/content/Hackathon-YZTA\"\n",
        "\n",
        "\n",
        "data_path = os.path.join(repo_path, 'data')\n",
        "\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    print(f\"Data klasörü bulundu: {data_path}\")\n",
        "else:\n",
        "    print(\"Data klasörü bulunamadı.\")"
      ],
      "metadata": {
        "id": "F-nrZmzp0GCV"
      },
      "id": "F-nrZmzp0GCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veriyi yüklemek için klasör yolunu kullanacağız\n",
        "train_dir = os.path.join(data_path, 'train')\n",
        "test_dir = os.path.join(data_path, 'test')"
      ],
      "metadata": {
        "id": "zPM6YEP50WuG"
      },
      "id": "zPM6YEP50WuG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def load_data_from_directories(train_dir, test_dir):\n",
        "    def read_from_category(base_path, category):\n",
        "        texts, labels = [], []\n",
        "        for label_name in [\"pos\", \"neg\"]:\n",
        "            label_value = 1 if label_name == \"pos\" else 0\n",
        "            folder = os.path.join(base_path, label_name)\n",
        "            for filename in os.listdir(folder):\n",
        "                if filename.endswith(\".txt\"):\n",
        "                    file_path = os.path.join(folder, filename)\n",
        "                    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                        texts.append(file.read())\n",
        "                        labels.append(label_value)\n",
        "        return pd.DataFrame({\"text\": texts, \"label\": labels})\n",
        "\n",
        "    train_df = read_from_category(train_dir, \"train\")\n",
        "    test_df = read_from_category(test_dir, \"test\")\n",
        "\n",
        "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    dataset = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(train_df),\n",
        "        \"test\": Dataset.from_pandas(test_df)\n",
        "    })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "dataset = load_data_from_directories(train_dir, test_dir)"
      ],
      "metadata": {
        "id": "b7SWR94O0eX0"
      },
      "id": "b7SWR94O0eX0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = \"dbmdz/bert-base-turkish-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "uwyD5pjf05Lw"
      },
      "id": "uwyD5pjf05Lw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "sHXcF-gv1TNn"
      },
      "id": "sHXcF-gv1TNn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=2)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sentiment_results\",\n",
        "    num_train_epochs=3,  # İstediğin epoch sayısını buraya yazabilirsin\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds)\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "l02IFo_i1Ze2"
      },
      "id": "l02IFo_i1Ze2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeli kaydetme\n",
        "model.save_pretrained('/content/drive/MyDrive/saved_model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/saved_model')"
      ],
      "metadata": {
        "id": "1s0tRjmv7vn6"
      },
      "id": "1s0tRjmv7vn6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
        "\n",
        "# Sonuçları yazdırma\n",
        "print(f\"Test doğruluğu: {results['eval_accuracy']}\")"
      ],
      "metadata": {
        "id": "Ilers_f_9erl"
      },
      "id": "Ilers_f_9erl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Cihazı otomatik belirle (GPU varsa kullan, yoksa CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Modeli cihaza taşı\n",
        "model.to(device)\n",
        "\n",
        "text = \"Bugün karbon izin az çıktı, bilinçlisin!\"\n",
        "\n",
        "# Metni tokenle ve cihaza taşı\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "# Tahmin al\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Sonuçları işle\n",
        "predicted_class_id = logits.argmax(dim=-1).item()\n",
        "labels = [\"neg\", \"pos\"]  # Sıralama senin modeline göre değişebilir\n",
        "\n",
        "predicted_label = labels[predicted_class_id]\n",
        "print(f\"Metin: {text}\")\n",
        "print(f\"Modelin tahmini: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "PAllFjYw91Li"
      },
      "id": "PAllFjYw91Li",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/saved_model.zip /content/drive/MyDrive/saved_model"
      ],
      "metadata": {
        "id": "u7J4MYbT5ImD"
      },
      "id": "u7J4MYbT5ImD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/saved_model.zip\")"
      ],
      "metadata": {
        "id": "YYQkv1h75VvB"
      },
      "id": "YYQkv1h75VvB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}